{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "Tras mostrar opciones para la detección y extracción de información de caras humanas con deep face, la tarea a entregar consiste en proponer un escenario de aplicación y desarrollar un prototipo de temática libre que provoque reacciones a partir de la información extraída del rostro. Los detectores proporcionan información del rostro, y de sus elementos faciales. Ideas inmediatas pueden ser filtros, aunque no hay limitaciones en este sentido. La entrega debe venir acompañada de un gif animado o vídeo de un máximo de 30 segundos con momentos seleccionados de la propuesta. Se utilizará para una posterior votación y elección de las mejores entre el grupo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Filter:</ins> Hats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "# Load hat images with transparency (alpha channel) from file paths\n",
    "hats = [\n",
    "    cv2.imread('./Images/No_trim/Hat 1.png', cv2.IMREAD_UNCHANGED), \n",
    "    cv2.imread('./Images/No_trim/Hat 2.png', cv2.IMREAD_UNCHANGED), \n",
    "    cv2.imread('./Images/No_trim/Hat 3.png', cv2.IMREAD_UNCHANGED),\n",
    "    cv2.imread('./Images/No_trim/Hat 4.png', cv2.IMREAD_UNCHANGED),\n",
    "    cv2.imread('./Images/No_trim/Hat 5.png', cv2.IMREAD_UNCHANGED),\n",
    "    cv2.imread('./Images/No_trim/Hat 6.png', cv2.IMREAD_UNCHANGED),\n",
    "    cv2.imread('./Images/No_trim/Hat 7.png', cv2.IMREAD_UNCHANGED),\n",
    "    cv2.imread('./Images/No_trim/Hat 8.png', cv2.IMREAD_UNCHANGED)\n",
    "]\n",
    "\n",
    "current_hat = 0  # Initialize the index of the current hat\n",
    "\n",
    "# Load dlib's face detector and facial landmark predictor\n",
    "p = \"./shape_predictor_68_face_landmarks.dat\"  # Path to the facial landmark model file\n",
    "detector = dlib.get_frontal_face_detector()    # Face detector using Histogram of Oriented Gradients (HOG)\n",
    "predictor = dlib.shape_predictor(p)            # Landmark predictor for facial feature detection\n",
    "\n",
    "# Start video capture from the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 640)  # Set video frame width\n",
    "cap.set(4, 480)  # Set video frame height\n",
    "\n",
    "# Main loop for capturing video frames and processing them\n",
    "while True:\n",
    "    # Capture a video frame and convert it to grayscale for face detection\n",
    "    _, image = cap.read()\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image_rgba = cv2.cvtColor(image, cv2.COLOR_BGR2BGRA)  # Add alpha channel for transparency\n",
    "    \n",
    "    # Detect faces in the grayscale image\n",
    "    rects = detector(gray, 0)\n",
    "    \n",
    "    # Loop through each detected face\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        shape = predictor(gray, rect)          # Predict facial landmarks\n",
    "        shape = face_utils.shape_to_np(shape)  # Convert landmarks to a NumPy array\n",
    "\n",
    "        # Estimate the top of the head based on forehead landmarks\n",
    "        top_head_x = int(np.mean(shape[0:27, 0]))  # X-coordinate\n",
    "        top_head_y = int(np.mean(shape[0:27, 1]))  # Y-coordinate\n",
    "\n",
    "        # Calculate the dimensions for resizing the hat\n",
    "        hat_width = int(1.5 * rect.width())  # Make hat proportional to face width\n",
    "        hat_height = hat_width * hats[current_hat].shape[0] // hats[current_hat].shape[1]  # Preserve hat aspect ratio\n",
    "\n",
    "        # Calculate the top-left corner for placing the resized hat image\n",
    "        x = top_head_x - hat_width // 2\n",
    "        y = top_head_y - hat_height\n",
    "\n",
    "        # Resize the hat to fit on the detected face\n",
    "        hat_resized = cv2.resize(hats[current_hat], (hat_width, hat_height))\n",
    "\n",
    "        # Overlay the hat onto the frame, using transparency if available\n",
    "        for i in range(hat_height):\n",
    "            for j in range(hat_width):\n",
    "                # Check boundaries and place non-transparent pixels\n",
    "                if 0 <= y + i < image.shape[0] and 0 <= x + j < image.shape[1] and hat_resized[i, j, 3] != 0:\n",
    "                    image_rgba[y + i, x + j] = hat_resized[i, j]\n",
    "    \n",
    "    # Display the frame with the hat overlay in a window\n",
    "    cv2.imshow(\"Hats Filter\", image_rgba)\n",
    "\n",
    "    # Wait for a key press and respond to it\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:             # Exit on 'ESC' key\n",
    "        break\n",
    "    elif k >= ord('1') and k <= ord('8'):  # Change hat with number keys (1 to 8)\n",
    "        current_hat = k - ord('1')\n",
    "\n",
    "# Release the video capture and close any open windows\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Filter:</ins> Beards and moustaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "# Load beard images with transparency (alpha channel) from file paths\n",
    "beards = [\n",
    "    cv2.imread('./Images/No_trim/Beard 1.png', cv2.IMREAD_UNCHANGED),\n",
    "    cv2.imread('./Images/No_trim/Beard 2.png', cv2.IMREAD_UNCHANGED),\n",
    "    cv2.imread('./Images/No_trim/Beard 3.png', cv2.IMREAD_UNCHANGED),\n",
    "    cv2.imread('./Images/No_trim/Beard 4.png', cv2.IMREAD_UNCHANGED),\n",
    "    cv2.imread('./Images/No_trim/Beard 5.png', cv2.IMREAD_UNCHANGED),\n",
    "    cv2.imread('./Images/No_trim/Beard 6.png', cv2.IMREAD_UNCHANGED),\n",
    "    cv2.imread('./Images/No_trim/Beard 7.png', cv2.IMREAD_UNCHANGED)\n",
    "]\n",
    "\n",
    "current_beard = 0  # Initialize the index of the current beard\n",
    "\n",
    "# Load dlib's face detector and facial landmark predictor\n",
    "p = \"./shape_predictor_68_face_landmarks.dat\"  # Path to the facial landmark model file\n",
    "detector = dlib.get_frontal_face_detector()    # Face detector using Histogram of Oriented Gradients (HOG)\n",
    "predictor = dlib.shape_predictor(p)            # Landmark predictor for facial feature detection\n",
    "\n",
    "# Start video capture from the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 640)  # Set video frame width\n",
    "cap.set(4, 480)  # Set video frame height\n",
    "\n",
    "# Main loop for capturing video frames and processing them\n",
    "while True:\n",
    "    # Capture a video frame and convert it to grayscale for face detection\n",
    "    _, image = cap.read()\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image_rgba = cv2.cvtColor(image, cv2.COLOR_BGR2BGRA)  # Add alpha channel for transparency\n",
    "    \n",
    "    # Detect faces in the grayscale image\n",
    "    rects = detector(gray, 0)\n",
    "    \n",
    "    # Loop through each detected face\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        shape = predictor(gray, rect)          # Predict facial landmarks\n",
    "        shape = face_utils.shape_to_np(shape)  # Convert landmarks to a NumPy array\n",
    "\n",
    "        # Estimate the top center of the face based on forehead landmarks\n",
    "        top_head_x = int(np.mean(shape[0:27, 0]))  # X-coordinate\n",
    "        top_head_y = int(np.mean(shape[0:27, 1]))  # Y-coordinate\n",
    "\n",
    "        # Calculate the dimensions for resizing the beard\n",
    "        beard_width = int(1.5 * rect.width())  # Make beard proportional to face width\n",
    "        beard_height = beard_width * beards[current_beard].shape[0] // beards[current_beard].shape[1]  # Preserve beard aspect ratio\n",
    "\n",
    "        # Calculate the position for placing the resized beard image\n",
    "        x = top_head_x - beard_width // 2\n",
    "        y = int(0.98 * shape[30][1])  # Align beard just below the nose\n",
    "\n",
    "        # Resize the beard to fit on the detected face\n",
    "        beard_resized = cv2.resize(beards[current_beard], (beard_width, beard_height))\n",
    "\n",
    "        # Overlay the beard onto the frame, using transparency if available\n",
    "        for i in range(beard_height):\n",
    "            for j in range(beard_width):\n",
    "                # Check boundaries and place non-transparent pixels\n",
    "                if 0 <= y + i < image.shape[0] and 0 <= x + j < image.shape[1] and beard_resized[i, j, 3] != 0:\n",
    "                    image_rgba[y + i, x + j] = beard_resized[i, j]\n",
    "    \n",
    "    # Display the frame with the beard overlay in a window\n",
    "    cv2.imshow(\"Beards and Moustaches Filter\", image_rgba)\n",
    "\n",
    "    # Wait for a key press and respond to it\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:             # Exit on 'ESC' key\n",
    "        break\n",
    "    elif k >= ord('1') and k <= ord('7'):  # Change beard with number keys (1 to 7)\n",
    "        current_beard = k - ord('1')\n",
    "\n",
    "# Release the video capture and close any open windows\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Filter:</ins> Lips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "# Load lip overlay images with transparency (alpha channel) from file paths\n",
    "lips_items = [\n",
    "    cv2.imread('./Images/No_trim/Lips 1.png', cv2.IMREAD_UNCHANGED),\n",
    "    cv2.imread('./Images/No_trim/Lips 2.png', cv2.IMREAD_UNCHANGED),\n",
    "    cv2.imread('./Images/No_trim/Lips 3.png', cv2.IMREAD_UNCHANGED)\n",
    "    # Optional additional images can be loaded here\n",
    "]\n",
    "\n",
    "current_item = 0  # Initialize the index of the current lip overlay\n",
    "\n",
    "# Initialize dlib's face detector and facial landmark predictor\n",
    "p = \"./shape_predictor_68_face_landmarks.dat\"  # Path to the facial landmark model file\n",
    "detector = dlib.get_frontal_face_detector()    # Face detector using Histogram of Oriented Gradients (HOG)\n",
    "predictor = dlib.shape_predictor(p)            # Landmark predictor for facial feature detection\n",
    "\n",
    "# Start video capture from the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 640)  # Set video frame width\n",
    "cap.set(4, 480)  # Set video frame height\n",
    "\n",
    "# Main loop for capturing video frames and processing them\n",
    "while True:\n",
    "    # Capture a video frame and convert it to grayscale for face detection\n",
    "    _, image = cap.read()\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the grayscale image\n",
    "    rects = detector(gray, 0)\n",
    "\n",
    "    # Loop through each detected face\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        shape = predictor(gray, rect)          # Predict facial landmarks\n",
    "        shape = face_utils.shape_to_np(shape)  # Convert landmarks to a NumPy array\n",
    "\n",
    "        # Get the coordinates of the lips (landmark points 48 to 67)\n",
    "        lips = shape[48:68]\n",
    "\n",
    "        # Calculate the center of the lips based on landmark points\n",
    "        lips_center = (int((lips[0][0] + lips[6][0]) / 2), int((lips[0][1] + lips[6][1]) / 2))\n",
    "\n",
    "        # Calculate the width and height for resizing the overlay image\n",
    "        overlay_width = int((lips[6][0] - lips[0][0]))  # Width based on lip landmarks\n",
    "        overlay_height = int(0.8 * overlay_width)        # Adjust height to maintain aspect ratio\n",
    "\n",
    "        # Resize the selected lip overlay image to fit the lips\n",
    "        overlay_image_resized = cv2.resize(lips_items[current_item], (overlay_width, overlay_height))\n",
    "\n",
    "        # Calculate the top-left position for placing the resized overlay image\n",
    "        x_pos = lips_center[0] - overlay_width // 2\n",
    "        y_pos = lips_center[1] - overlay_height // 2\n",
    "\n",
    "        # Check if the overlay image is within the bounds of the original image\n",
    "        if x_pos >= 0 and y_pos >= 0 and x_pos + overlay_width <= image.shape[1] and y_pos + overlay_height <= image.shape[0]:\n",
    "            # Create a mask based on the alpha channel of the overlay image\n",
    "            mask = overlay_image_resized[:, :, 3] / 255.0  # Normalize alpha channel to range [0, 1]\n",
    "            mask = cv2.merge([mask, mask, mask])           # Convert to 3-channel mask for blending\n",
    "\n",
    "            # Extract the RGB region of the overlay image for blending\n",
    "            overlay_region = overlay_image_resized[:, :, 0:3]\n",
    "\n",
    "            # Apply the overlay image to the original image using the mask for blending\n",
    "            image[y_pos:y_pos + overlay_height, x_pos:x_pos + overlay_width] = \\\n",
    "                (1 - mask) * image[y_pos:y_pos + overlay_height, x_pos:x_pos + overlay_width] + mask * overlay_region\n",
    "\n",
    "    # Display the image with the lip overlay applied\n",
    "    cv2.imshow(\"Lips Filter\", image)\n",
    "\n",
    "    # Wait for a key press and respond to it\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:             # Exit on 'ESC' key\n",
    "        break\n",
    "    elif k >= ord('1') and k <= ord('3'):  # Change lip overlay with number keys (1 to 3)\n",
    "        current_item = k - ord('1')\n",
    "\n",
    "# Release the video capture and close any open windows\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Filter:</ins> Glasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "# Initialize dlib's face detector and create a facial landmark predictor\n",
    "p = \"./shape_predictor_68_face_landmarks.dat\"  # Path to pre-trained landmark model\n",
    "detector = dlib.get_frontal_face_detector()    # Face detector using Histogram of Oriented Gradients (HOG)\n",
    "predictor = dlib.shape_predictor(p)            # Landmark predictor for detecting facial features\n",
    "\n",
    "# Load glasses overlay images with transparency (alpha channel) from file paths\n",
    "glasses_items = [\n",
    "    cv2.imread(\"./Images/No_trim/Glasses 1.png\", cv2.IMREAD_UNCHANGED),\n",
    "    cv2.imread('./Images/No_trim/Glasses 2.png', cv2.IMREAD_UNCHANGED),\n",
    "    cv2.imread('./Images/No_trim/Glasses 3.png', cv2.IMREAD_UNCHANGED),\n",
    "]\n",
    "\n",
    "current_item = 0  # Initialize the index of the current glasses overlay\n",
    "\n",
    "# Start video capture from the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Main loop to capture video frames and apply the glasses filter\n",
    "while True:\n",
    "    # Capture a video frame and convert it to grayscale for face detection\n",
    "    _, image = cap.read()\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the grayscale image\n",
    "    rects = detector(gray, 0)\n",
    "\n",
    "    # Loop through each detected face\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        # Predict facial landmarks for the detected face region\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)  # Convert landmarks to a NumPy array\n",
    "\n",
    "        # Get the coordinates for each eye\n",
    "        left_eye = shape[42:48]\n",
    "        right_eye = shape[36:42]\n",
    "\n",
    "        # Calculate the center of each eye\n",
    "        left_eye_center = np.mean(left_eye, axis=0).astype(int)\n",
    "        right_eye_center = np.mean(right_eye, axis=0).astype(int)\n",
    "\n",
    "        # Calculate the midpoint between the eyes to center the glasses\n",
    "        eyes_center = ((left_eye_center[0] + right_eye_center[0]) // 2, (left_eye_center[1] + right_eye_center[1]) // 2)\n",
    "\n",
    "        # Calculate the scale factor for resizing the glasses based on the distance between the eyes\n",
    "        distance = abs(left_eye_center[0] - right_eye_center[0])\n",
    "        scale_factor = 2.0 * distance / glasses_items[current_item].shape[1]  # Scale factor to fit glasses width to eye distance\n",
    "\n",
    "        # Resize the selected glasses overlay to fit between the eyes\n",
    "        new_glasses = cv2.resize(glasses_items[current_item], (0, 0), fx=scale_factor, fy=scale_factor)\n",
    "\n",
    "        # Calculate the position to place the glasses image centered at the eyes\n",
    "        x_offset = eyes_center[0] - new_glasses.shape[1] // 2\n",
    "        y_offset = eyes_center[1] - new_glasses.shape[0] // 2\n",
    "\n",
    "        # Overlay the glasses image on the face with transparency blending\n",
    "        for c in range(0, 3):  # Iterate through the color channels (B, G, R)\n",
    "            image[y_offset:y_offset + new_glasses.shape[0], x_offset:x_offset + new_glasses.shape[1], c] = (\n",
    "                image[y_offset:y_offset + new_glasses.shape[0], x_offset:x_offset + new_glasses.shape[1], c] * (1 - new_glasses[:, :, 3] / 255.0) +\n",
    "                new_glasses[:, :, c] * (new_glasses[:, :, 3] / 255.0)\n",
    "            )\n",
    "\n",
    "    # Display the resulting image with the glasses overlay\n",
    "    cv2.imshow(\"Glasses Filter\", image)\n",
    "    \n",
    "    # Check for key presses to exit or switch the glasses overlay\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:  # Exit loop if 'ESC' key is pressed\n",
    "        break\n",
    "    elif k >= ord('1') and k <= ord('3'):  # Switch glasses with number keys (1 to 3)\n",
    "        current_item = k - ord('1')\n",
    "\n",
    "# Release the video capture and close any open windows\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Filter:</ins> Noses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from imutils import face_utils\n",
    "from math import hypot\n",
    "import numpy as np\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "# Initialize the video capture (webcam) and load the nose image overlay\n",
    "cap = cv2.VideoCapture(0)\n",
    "nose_image = cv2.imread(\"./Images/Noses/Noses 1.png\")  # Load overlay image for nose\n",
    "\n",
    "# Read an initial frame to create a mask of the same size as the video frames\n",
    "_, frame = cap.read()\n",
    "rows, cols, _ = frame.shape\n",
    "nose_mask = np.zeros((rows, cols), np.uint8)  # Empty mask to hold nose overlay region\n",
    "\n",
    "# Initialize dlib's face detector and facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"./shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Main loop to capture video frames and apply the nose filter\n",
    "while True:\n",
    "    # Capture a video frame\n",
    "    _, frame = cap.read()\n",
    "    \n",
    "    # Clear the mask for each frame and convert the frame to grayscale\n",
    "    nose_mask.fill(0)\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the current frame\n",
    "    faces = detector(frame)\n",
    "    for face in faces:\n",
    "        # Predict facial landmarks for each detected face\n",
    "        landmarks = predictor(gray_frame, face)\n",
    "\n",
    "        # Define coordinates for the nose landmarks\n",
    "        top_nose = (landmarks.part(29).x, landmarks.part(29).y)\n",
    "        center_nose = (landmarks.part(30).x, landmarks.part(30).y)\n",
    "        left_nose = (landmarks.part(31).x, landmarks.part(31).y)\n",
    "        right_nose = (landmarks.part(35).x, landmarks.part(35).y)\n",
    "\n",
    "        # Calculate the width and height for the nose overlay\n",
    "        nose_width = int(hypot(left_nose[0] - right_nose[0], left_nose[1] - right_nose[1]) * 1.7)\n",
    "        nose_height = int(nose_width * 0.77)\n",
    "\n",
    "        # Determine the top-left and bottom-right points for placing the overlay\n",
    "        top_left = (int(center_nose[0] - nose_width / 2), int(center_nose[1] - nose_height / 2))\n",
    "        bottom_right = (int(center_nose[0] + nose_width / 2), int(center_nose[1] + nose_height / 2))\n",
    "\n",
    "        # Resize the nose overlay to fit the detected nose area\n",
    "        nose_pig = cv2.resize(nose_image, (nose_width, nose_height))\n",
    "\n",
    "        # Create a grayscale version of the nose overlay for mask creation\n",
    "        nose_pig_gray = cv2.cvtColor(nose_pig, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Create a binary mask where the non-nose pixels are transparent\n",
    "        _, nose_mask = cv2.threshold(nose_pig_gray, 25, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "        # Select the region of interest on the frame for overlay placement\n",
    "        nose_area = frame[top_left[1]: top_left[1] + nose_height, top_left[0]: top_left[0] + nose_width]\n",
    "\n",
    "        # Apply the mask to remove the original nose region\n",
    "        nose_area_no_nose = cv2.bitwise_and(nose_area, nose_area, mask=nose_mask)\n",
    "\n",
    "        # Combine the mask and overlay to create the final nose\n",
    "        final_nose = cv2.add(nose_area_no_nose, nose_pig)\n",
    "\n",
    "        # Place the final nose on the frame\n",
    "        frame[top_left[1]: top_left[1] + nose_height, top_left[0]: top_left[0] + nose_width] = final_nose\n",
    "\n",
    "    # Display the final frame with the nose overlay\n",
    "    cv2.imshow(\"Noses Filter\", frame)\n",
    "\n",
    "    # Check for key press to exit the loop\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:  # Exit loop if 'ESC' key is pressed\n",
    "        break\n",
    "\n",
    "# Release the video capture and close any open windows\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Filter:</ins> Glitch eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from imutils import face_utils, translate, resize\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import argparse\n",
    "import dlib\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# Set the path to the pre-trained facial landmark predictor model\n",
    "predictor_path = \"./shape_predictor_68_face_landmarks.dat\"  # Replace with actual path to the predictor\n",
    "\n",
    "print(\"starting program.\")\n",
    "print(\"'s' starts drawing eyes.\")\n",
    "print(\"'r' to toggle recording image, and 'q' to quit\")\n",
    "\n",
    "# Initialize the webcam and allow it to warm up\n",
    "vs = VideoStream().start()\n",
    "time.sleep(1.5)\n",
    "\n",
    "# Initialize dlib's face detector and shape predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "recording = False   # Toggle for recording images\n",
    "counter = 0         # Counter for saved images\n",
    "\n",
    "# Class to manage list of past eye positions for \"eye trail\" effect\n",
    "class EyeList(object):\n",
    "    def __init__(self, length):\n",
    "        self.length = length\n",
    "        self.eyes = []\n",
    "\n",
    "    def push(self, newcoords):\n",
    "        if len(self.eyes) < self.length:\n",
    "            self.eyes.append(newcoords)\n",
    "        else:\n",
    "            self.eyes.pop(0)\n",
    "            self.eyes.append(newcoords)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.eyes = []\n",
    "\n",
    "# Initialize EyeList with a trail length of 10 frames\n",
    "eyelist = EyeList(10)\n",
    "eyeSnake = False  # Toggle for eye trail effect\n",
    "\n",
    "# Capture an initial frame to determine the resolution\n",
    "frame = vs.read()\n",
    "frame = resize(frame, width=800)\n",
    "\n",
    "# Initialize layers and masks for eye rendering\n",
    "eyelayer = np.zeros(frame.shape, dtype='uint8')\n",
    "eyemask = eyelayer.copy()\n",
    "eyemask = cv2.cvtColor(eyemask, cv2.COLOR_BGR2GRAY)\n",
    "translated = np.zeros(frame.shape, dtype='uint8')\n",
    "translated_mask = eyemask.copy()\n",
    "\n",
    "# Main loop for real-time processing\n",
    "while True:\n",
    "    # Capture a frame from the webcam and resize it\n",
    "    frame = vs.read()\n",
    "    frame = resize(frame, width=800)\n",
    "\n",
    "    # Reset all layers and masks\n",
    "    eyelayer.fill(0)\n",
    "    eyemask.fill(0)\n",
    "    translated.fill(0)\n",
    "    translated_mask.fill(0)\n",
    "\n",
    "    # Convert frame to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    rects = detector(gray, 0)\n",
    "\n",
    "    # If \"eyeSnake\" effect is enabled\n",
    "    if eyeSnake:\n",
    "        for rect in rects:\n",
    "            # Detect facial landmarks\n",
    "            shape = predictor(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "            # Define points for left and right eyes\n",
    "            leftEye = shape[36:42]\n",
    "            rightEye = shape[42:48]\n",
    "\n",
    "            # Fill masks for left and right eyes\n",
    "            cv2.fillPoly(eyemask, [leftEye], 255)\n",
    "            cv2.fillPoly(eyemask, [rightEye], 255)\n",
    "\n",
    "            # Copy eyes area to the eyelayer using the mask\n",
    "            eyelayer = cv2.bitwise_and(frame, frame, mask=eyemask)\n",
    "\n",
    "            # Determine bounding rectangle around the eye mask\n",
    "            x, y, w, h = cv2.boundingRect(eyemask)\n",
    "\n",
    "            # Add current eye position to EyeList\n",
    "            eyelist.push([x, y])\n",
    "\n",
    "            # Draw past eye positions in reverse order\n",
    "            for i in reversed(eyelist.eyes):\n",
    "                # Translate the eye layer and mask to past positions\n",
    "                translated1 = translate(eyelayer, i[0] - x, i[1] - y)\n",
    "                translated1_mask = translate(eyemask, i[0] - x, i[1] - y)\n",
    "                \n",
    "                # Update the mask and add the translated eye layer\n",
    "                translated_mask = np.maximum(translated_mask, translated1_mask)\n",
    "                translated = cv2.bitwise_and(translated, translated, mask=255 - translated1_mask)\n",
    "                translated += translated1\n",
    "\n",
    "        # Apply the translated eyes overlay to the frame\n",
    "        frame = cv2.bitwise_and(frame, frame, mask=255 - translated_mask)\n",
    "        frame += translated\n",
    "\n",
    "    # Display the resulting frame with the eye effect\n",
    "    cv2.imshow(\"Glitch Eyes Filter\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # Record frames to disk if recording is enabled\n",
    "    if recording:\n",
    "        cv2.imwrite(\"image_seq/%05d.png\" % counter, frame)\n",
    "        counter += 1\n",
    "\n",
    "    # Toggle recording or effects based on key presses\n",
    "    if key == ord(\"q\"):  # Quit program\n",
    "        break\n",
    "    if key == ord(\"s\"):  # Toggle eye trail effect\n",
    "        eyeSnake = not eyeSnake\n",
    "        eyelist.clear()\n",
    "    if key == ord(\"r\"):  # Toggle recording\n",
    "        recording = not recording\n",
    "\n",
    "# Release resources\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Filter:</ins> Face detection modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "# Start video capture from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize dlib's face detector (HOG-based) and shape predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"./shape_predictor_68_face_landmarks.dat\")  # Path to facial landmarks model\n",
    "\n",
    "# Main loop for real-time video processing\n",
    "while True:\n",
    "    # Capture a frame from the webcam\n",
    "    _, frame = cap.read()\n",
    "    \n",
    "    # Convert the frame to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the grayscale frame\n",
    "    faces = detector(gray)\n",
    "    \n",
    "    # Loop through each detected face\n",
    "    for face in faces:\n",
    "        # Extract the coordinates of the face rectangle\n",
    "        x1 = face.left()    # Left corner x-coordinate\n",
    "        y1 = face.top()     # Top corner y-coordinate\n",
    "        x2 = face.right()   # Right corner x-coordinate\n",
    "        y2 = face.bottom()  # Bottom corner y-coordinate\n",
    "        \n",
    "        # Uncomment to draw a rectangle around each face\n",
    "        # cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "\n",
    "        # Predict facial landmarks within the face rectangle\n",
    "        landmarks = predictor(gray, face)\n",
    "\n",
    "        # Loop through the 68 facial landmarks\n",
    "        for n in range(0, 68):\n",
    "            x = landmarks.part(n).x  # x-coordinate of the landmark\n",
    "            y = landmarks.part(n).y  # y-coordinate of the landmark\n",
    "            \n",
    "            # Draw a small circle at each landmark position\n",
    "            cv2.circle(frame, (x, y), 4, (255, 0, 0), -1)  # Blue circle for each landmark\n",
    "\n",
    "    # Display the frame with face landmarks\n",
    "    cv2.imshow(\"Face Detection Modeling\", frame)\n",
    "\n",
    "    # Check for the 'Esc' key to exit the loop\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:  # ASCII code for 'Esc' key\n",
    "        break\n",
    "\n",
    "# Release video capture and close display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mi_entorno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
